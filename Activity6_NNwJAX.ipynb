{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LINdrlJYoR2f"
   },
   "source": [
    "# Neural networks with Jax!\n",
    "\n",
    "![img](https://jax.readthedocs.io/en/latest/_static/jax_logo_250px.png)\n",
    "\n",
    "Jax is an efficient computation package (think `numpy` with coprocessors) that includes automatic differentiation of functions. Jax has become quite popular in recent years for many different fields, including machine learning.\n",
    "\n",
    "Let's start by exploring the parts of the package that are useful for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viRW3aJxsDUU"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0ZEnS62o0tG"
   },
   "source": [
    "Let's build a simple function for\n",
    "\n",
    "$$f(x) = \\sin{3x}$$\n",
    "\n",
    "This is a function that we (hopefully) know the derivative for, so let's also code a function that returns the analytic derivative for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDaN0XKypD4s"
   },
   "outputs": [],
   "source": [
    "# standard way of defining a python function\n",
    "def f(x):\n",
    "    return jnp.sin(3*x) # using jax numpy operations\n",
    "\n",
    "# define a function for the analytic derivative (not using automatic differentiation)\n",
    "def fprime(x):\n",
    "    return 3*jnp.cos(3*x) # still can use jax numpy for faster operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dty16cHRpJ-f"
   },
   "source": [
    "Now, let's use the automatic differentiation engine within Jax and compare it to a known derivative using `jax.grad()`\n",
    "\n",
    "https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#taking-derivatives-with-grad#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wZ9Or6lEpPk6",
    "outputId": "5097cecf-4aa9-47ca-ddc1-1f7c22271eab"
   },
   "outputs": [],
   "source": [
    "# transform the f(x) function into a function for f(x)'s gradient\n",
    "diff_f = jax.grad(f)\n",
    "\n",
    "print(\"autodiff derivative:\", diff_f(5.))\n",
    "print(\"analytic dericative:\", fprime(5.))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRltI13YqN3S"
   },
   "source": [
    "You should observe that the autodiff result is *exact* to numerical roundoff. That is because automatic differentiation is an analytically exact method, not a numeric method. Try modifying the above cells with a different function.\n",
    "\n",
    "Notice that `jax.grad` transforms a fuction. Therefore, `diff_f(x)` is a tranformed function that computes the gradient (e.g. derivative), of `f(x)`. The independent variables for the gradient are the *first argument* of the function by default, in the case of multiple arguments.\n",
    "\n",
    "---\n",
    "\n",
    "## Now, let's create some data for a regression task.\n",
    "\n",
    "I will create a synthetic dataset with three features that map to our ground truth outputd $y$ via the function\n",
    "$$y = .5c + .2x + 2*x +Ïƒ,$$ where $Ïƒ$ is noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83uhPMwLteDQ"
   },
   "outputs": [],
   "source": [
    "# define our synthetic data\n",
    "N = 1000 # number of examples\n",
    "n_feat = 3\n",
    "x = np.random.uniform(size=(N,n_feat))\n",
    "\n",
    "def true(x):\n",
    "    return .5*x[:,0] + .2*x[:,1] + 2*x[:,2] + np.random.normal(scale=.1)\n",
    "\n",
    "y = true(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Od_gfP63tc05"
   },
   "outputs": [],
   "source": [
    "#checking that the data looks as expected\n",
    "#plt.plot(x[:,0], y,'.')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YD2G4LYAskRo"
   },
   "source": [
    "## Let's first build a linear regression model, and find the optimal model parameters via gradient descent.\n",
    "\n",
    "We know that the data follows a linear trend, **and** we know the coefficients. This is a great testing ground for this package. \n",
    "\n",
    "We are going to build the same algorithm as we did for linear regression via gradient descent at the beginning of the semester, but we will use automatic differentiation for computing gradients.\n",
    "\n",
    "To maximize the efficiency of the I am going to build a model that will simultaneouly predict on the entire training dataset. To do this, I need some linear algebra.\n",
    "\n",
    "\n",
    "$ \\mathbf{x} $ with 4 examples and 3 features, is represented as:\n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix}\n",
    "x_{11} & x_{12} & x_{13} \\\\\n",
    "x_{21} & x_{22} & x_{23} \\\\\n",
    "x_{31} & x_{32} & x_{33} \\\\\n",
    "x_{41} & x_{42} & x_{43}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "And our model weights vector\n",
    "$$\n",
    "\\mathbf{w} = \\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "w_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The model $$ \\mathbf{y_{pred}} = \\mathbf{x} \\cdot \\mathbf{w} $$ will provide a $4 \\times 1$ array of predictions, one for each example.\n",
    "\n",
    "$$\n",
    "\\mathbf{y_{pred}} = \\begin{bmatrix}\n",
    "w_1x_{11} + w_2x_{12} + w_3x_{13} \\\\\n",
    "w_1x_{21} + w_2x_{22} + w_3x_{23} \\\\\n",
    "w_1x_{31} + w_2x_{32} + w_3x_{33}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "(*Note: I left out the bias term, which is not good practice but ok for our simple example because the ground truth model has no bias.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOy3-0Y8srDe"
   },
   "outputs": [],
   "source": [
    "# Define the linear regression model described above\n",
    "def model(w,x):\n",
    "    return jnp.dot(x,w) #using jax operations\n",
    "\n",
    "# Define the mean squared error loss function\n",
    "# jnp.mean() can help you here\n",
    "def loss(w,x,y_tr):\n",
    "    #complete me\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "# Define the gradient of the loss function (above example can help here)\n",
    "\n",
    "\n",
    "# Perform gradient descent to optimize the parameters\n",
    "# and keep track of the loss on the training data and the validation data\n",
    "def gradient_descent(w, xtrain, ytrain, xval, yval, learning_rate=0.1, num_epochs=100):\n",
    "    loss_f = np.zeros(num_epochs)\n",
    "    valloss_f = np.zeros(num_epochs)\n",
    "    for i in range(num_epochs):\n",
    "        # compute the gradient (complete me)\n",
    "\n",
    "        \n",
    "        # add losses for plotting\n",
    "        loss_f[i] = loss(w,xtrain,ytrain)\n",
    "        valloss_f[i] = loss(w,xval,yval)\n",
    "        # update weights (complete me)\n",
    "        \n",
    "        \n",
    "    return w, loss_f, valloss_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "RKCorNw0sGj7",
    "outputId": "720a9f9a-d9b2-4bf9-8b7b-29a55ea32562"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize parameters randomly\n",
    "key = jax.random.PRNGKey(5) ## seeding the RNG\n",
    "w = jax.random.normal(key, (3,))\n",
    "split = int((N*.8)//1) # train/val split\n",
    "\n",
    "# yah, I could have done this better using sklearn :)]\n",
    "# training data\n",
    "train_x = x[:split] \n",
    "train_y = y[:split]\n",
    "# val data\n",
    "val_x = x[split:]\n",
    "val_y = y[split:]\n",
    "# make a test set for final performance metric\n",
    "\n",
    "\n",
    "# Perform gradient descent\n",
    "w, lossf, val_lossf = gradient_descent(w, train_x, train_y, val_x, val_y, learning_rate=.1, num_epochs=500)\n",
    "plt.plot(lossf, label=\"train loss\")\n",
    "plt.plot(val_lossf, label=\"val loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Print the learned parameters\n",
    "print(\"Learned parameters:\", w)\n",
    "\n",
    "# Test the model with test data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Awesome! You should have gotten decent results, and the code should have been simpler and faster to execute than our first go at linear regression.\n",
    " ---\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "Let's now apply what we have learned to build a simple regression neural network with 1 hidden layer of 4 nodes.\n",
    "\n",
    "The trick here is to create a 2D **matrix** of weights, where number of rows is the number weights in the model and the number of columns is the number of nodes in the layer. This allows us to operate on the entire *layer* at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ow2QlpBqspgR"
   },
   "outputs": [],
   "source": [
    "# Build our neural network layers\n",
    "\n",
    "# sigmoid activation for our hidden layer\n",
    "def sigmoid_layer(sig_w,x):\n",
    "    #using jax operations: jnp.dot(x,w)\n",
    "    \n",
    "    return\n",
    "\n",
    "# linear activation for our output layer (for a regression model)\n",
    "def linear(lin_w,inputs):\n",
    "    #again use jnp.dot(x,w)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def nn_model(ws,x):\n",
    "    # get weights for each layer\n",
    "    w_sig = ws[0]\n",
    "    w_lin = ws[1]\n",
    "    # complete me\n",
    "\n",
    "  return y_pred\n",
    "\n",
    "\n",
    "# Define the mean squared error loss function\n",
    "def nn_loss(w,x,y):\n",
    "    # complete me\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "# Define the gradient of the loss function\n",
    "\n",
    "# Perform gradient descent to optimize the parameters\n",
    "def nn_gradient_descent(w, xtrain, ytrain, xval, yval, learning_rate=.1, num_epochs=1000):\n",
    "    w1, w2 = w\n",
    "    #print(\"ws\",w1,w2)\n",
    "    loss_f = np.zeros(num_epochs)\n",
    "    valloss_f = np.zeros(num_epochs)\n",
    "    for i in range(num_epochs):\n",
    "        # complete me\n",
    "    \n",
    "        w = [w1,w2]\n",
    "        \n",
    "\n",
    "        loss_f[i] = nn_loss(w,xtrain,ytrain)\n",
    "        valloss_f[i] = nn_loss(w,xval,yval)\n",
    "\n",
    "    return w, loss_f, valloss_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow your linear regression example to train your (maybe) first neural network!\n",
    "\n",
    "We will use the same data as our linear model above. I have initialized the neural network weights for you, but the rest is up to you!\n",
    "\n",
    "Make sure to plot your loss curve and make predictions on your test set.\n",
    "\n",
    "Play around! Change hyperparameters, change the data, have fun! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "K2Tu9W8ntCAY",
    "outputId": "a994fc68-6246-4aeb-dc7b-9ddcd06d6c8b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize parameters randomly\n",
    "key = jax.random.PRNGKey(22)\n",
    "num_features = len(x[0])\n",
    "num_hidden = 3\n",
    "\n",
    "# layer 1: num features * num nodes\n",
    "w1 = jax.random.normal(key, (num_features, num_hidden))\n",
    "\n",
    "# layer 2: num inputs * num nodes (1)\n",
    "w2 = jax.random.normal(key, (3,))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAwczIzbwVnl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python-3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
