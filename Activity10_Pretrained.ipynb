{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd5986d9-3e22-4763-abc8-327b6f1035cb",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks in pyTorch\n",
    "\n",
    "Architecture design becomes more complicated as we add in more types of layers. We can take advantage of successful architectures and their weights by importing *pretrained models*.\n",
    "\n",
    "In this activity, we will load well-known CNN architectures and their pre-trained weights, and tune them for classification of the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. \n",
    "\n",
    "We will start by loading in a ResNet CNN architecture, which was first introduced in [this paper](https://arxiv.org/abs/1512.03385). They introduce a small mathematical trick which allows for less trainable parameters than earlier CNNs, which is useful as we are all sharing GPU resources :).\n",
    "\n",
    "## To Do:\n",
    "#### 1. Take a moment to read about the model linked above. Figure 3 compares the ResNet architecture to VGG, a more standard CNN architecture. We will load in ResNet18, an 18-layer version of this model.\n",
    "\n",
    "#### 2. In Section 3.4 note that the authors preprocessed ImageNet (RGB) data into a shape of $224 \\times 224 \\times 3$ pixels per image. Our first task is to map our image data into the same shape so we can use the architecture without much modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c36823a9-051c-4829-8e55-159ac72fbee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "from torchvision.io import decode_image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a63a84-ca8f-4a6f-b5a7-b062ed65215f",
   "metadata": {},
   "source": [
    "To accomplish this, we can use the `transforms.Compose` method from `torchvision`. I've done this for you here.\n",
    "\n",
    "*Note: I didn't find the scaling used on the images in the paper. A simple testing of a couple scalings led me to choose the one here... it may not be the best!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d37a31ef-1f27-4f54-b269-7976cd5583e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # ResNet expects at least 224x224 images\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel RGB format\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # (mean, std)\n",
    "])\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fecddcf-a2b9-4b54-b202-1efd90835c30",
   "metadata": {},
   "source": [
    "### 3. Visualize the images to make sure you understand the data. (E.g. what do you expect to look different from last time?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a401f767-0500-4a86-98c0-7d1445070e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Class labels that associate with numeric labels in dataset: 0,1,2,3,4,5,6,7,8,9\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b7a6e8-2c23-451c-b0df-5544ccb8e3ad",
   "metadata": {},
   "source": [
    "### 4. We now load in the resnet18 model with it's pretrained weights from the paper. Look through the architecture\n",
    "\n",
    "Note that there is only one \"Dense\" or fully connected layer in this architecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a38f867-72e7-4c7a-91a7-6620c226627d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_res = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1) # need to explicitly import and load weights\n",
    "print(model_res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f7d57-8596-4ddf-a192-b1f29bc175fe",
   "metadata": {},
   "source": [
    "### 5. We then re-define the last layer of the model for *our* classification task.\n",
    "\n",
    "Check your new architecture and check that it is what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0952f380-7e57-4a29-90a1-308b427b2531",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2235788832.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(model_res)\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "num_features = model_res.fc.in_features # how many values going into last layer?\n",
    "model_res.fc = nn.Linear(# complete me)\n",
    "print(model_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111158a7-c54b-4ff7-ae6b-041cc33c9497",
   "metadata": {},
   "source": [
    "### 6. We can choose to only train the last layer or let all the layers fine tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "009de024-f82b-499c-b5c6-33e9028501dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start by just training the final layer:\n",
    "for param in model_res.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_res.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e7daf57-c42b-49db-9ccc-5d1cab42dfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Epoch 1, Loss: 0.6011\n",
      "Epoch 2, Loss: 0.3849\n",
      "Epoch 3, Loss: 0.3469\n",
      "Epoch 4, Loss: 0.4370\n",
      "Epoch 5, Loss: 0.3912\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# am i using a GPU? This is so slow :(\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "model = model_res.to(device)\n",
    "\n",
    "loss_f = # complete me\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=1e-4) # good starting point\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(5):  # Keep it short for the activity\n",
    "    model_res.train()\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model_res(images)\n",
    "        loss = loss_f(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e64366-4f31-4a1a-b81d-d6bf91a09c24",
   "metadata": {},
   "source": [
    "### 5. Evaluate your model as before. Then, try to tune the model for increased performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f24f8f-4f1d-4b3e-9298-83f5644a70dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa1902fc-c465-441a-84c7-999ad1e6b35c",
   "metadata": {},
   "source": [
    "### 6. If you have time, you can swap out the model architecture for another pre-trained model. Check the `torch` documentation to find all the available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40d702b-ca2b-4bff-8266-7ca35dab07bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python-3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
