{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks / Autoencoders with PyTorch\n",
    "\n",
    "PyTorch is one of the leading packages for deep learning in industry today. We will first re-build our same Jax neural network with torch, then we will build an autoencoder.\n",
    "\n",
    "\n",
    "## Part 1: Simple Neural Network on Synthetic Data\n",
    "In this section, you will implement a small feedforward neural network with **one hidden layer** to predict from synthetic data.\n",
    "\n",
    "### **Task:**\n",
    "1. Generate synthetic data.\n",
    "2. Build a neural network with **one hidden layer** (4 neurons, sigmoid activation).\n",
    "3. Train it and observe its performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We first generate the same synthetic data as before. `pyTorch` and `numpy` us different default float precision, so we will use single precision, `pyTorch`'s default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our synthetic data\n",
    "N = 1000 # number of examples\n",
    "n_feat = 3\n",
    "x = np.random.uniform(size=(N,n_feat))\n",
    "\n",
    "def true(x):\n",
    "    return .5*x[:,0] + .2*x[:,1] + 2*x[:,2] + np.random.normal(scale=.1)\n",
    "\n",
    "y = true(x)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X = torch.from_numpy(x.astype(np.float32))\n",
    "y = torch.from_numpy(y.astype(np.float32)).unsqueeze(1)  # need shape (N x 1) instead of (1 x N)\n",
    "\n",
    "#janky train/val/test splitting\n",
    "X_train = X[:600]\n",
    "y_train = y[:600]\n",
    "X_val = X[600:800]\n",
    "y_val = y[600:800]\n",
    "X_test = X[800:]\n",
    "y_test = y[800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the NN using `pyTorch`\n",
    "\n",
    "We will create our same neural network as before. This is a regression network with one hidden layer with 4 neurons and **Sigmoid** activation function.\n",
    "\n",
    "Step through this example carefully to make sure you understand the pieces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network using PyTorch's nn.Module\n",
    "class SimpleNN(nn.Module): #inherit from nn Module\n",
    "    def __init__(self):\n",
    "        # Call the __init__ method of the base class nn.Module\n",
    "        super(SimpleNN, self).__init__()\n",
    "\n",
    "        # Define the network structure using nn.Sequential\n",
    "        # This stacks layers together in the order they are written\n",
    "        self.net = nn.Sequential(\n",
    "            # First layer: Fully connected (Linear) layer\n",
    "            # Maps input of size \"n_feat\" (which is 3 features in this dataset) to 4 hidden neurons\n",
    "            nn.Linear(n_feat, 4),\n",
    "\n",
    "            # Activation function: Sigmoid\n",
    "            # Applied to the output of the linear layer above\n",
    "            nn.Sigmoid(),\n",
    "\n",
    "            # Last layer: \n",
    "            #nn.#complete me\n",
    "        )\n",
    "\n",
    "    # Forward method defines how data passes through the model\n",
    "    def forward(self, x):\n",
    "        # Pass input x through the defined object \"net\" (the layers above)\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the neural network\n",
    "\n",
    "Again, step through this example carefully and ask questions when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of our neural network\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define the loss function: Mean Squared Error (MSE)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=.1) # SGD = stochastic gradient descent\n",
    "\n",
    "loss_train = []\n",
    "\n",
    "for epoch in range(5000):\n",
    "    #zero out our grandients for this pass\n",
    "    optimizer.zero_grad()\n",
    "    # predict on train data\n",
    "    output = model(X_train)\n",
    "    # compute train loss\n",
    "    loss = loss_function(output, y_train)\n",
    "    loss_train.append(loss.item())\n",
    "    # predict and compute loss on val data\n",
    "    with torch.no_grad(): # No need to compute gradients for validation data\n",
    "        out_val = #complete me\n",
    "        # keep track val loss\n",
    "    # backprop, autodifferentiation\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# plot loss curves here\n",
    "\n",
    "# complete me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on test data:\n",
    "\n",
    "Make predictions and plot predicted y values versus true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = # complete me\n",
    "\n",
    "# make plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Autoencoders\n",
    "\n",
    "We will build an autoencoder for the popular [MNIST](https://en.wikipedia.org/wiki/MNIST_database#:~:text=The%20MNIST%20database%20(Modified%20National,the%20field%20of%20machine%20learning.) dataset, which is a dataset of processed handwritten digits 0-9.\n",
    "\n",
    "### **Task:**\n",
    "1. Data exploration and processing\n",
    "2. Build an autoencoder neural network with **one hidden layer** (4 neurons, sigmoid activation).\n",
    "3. Train it and observe its performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data processing\n",
    "\n",
    "Fist we read in the data, then visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # let's try to to Part 2 on GPU\n",
    "\n",
    "# Load MNIST dataset.. it's built into torch!\n",
    "transform = transforms.ToTensor() # let's get the data into pytorch tensor form\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=False) \n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=False) \n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # we are going to batch our data due to the size of the data (e.g. we will do stochastic gradient descent!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)\n",
    "print(test_dataset)\n",
    "image, label = train_dataset[0] # get one example\n",
    "#print(np.array(image)) # look at contents of one example\n",
    "\n",
    "# now plot a few examples:\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(25):\n",
    "    image, label = train_dataset[i]\n",
    "    \n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(train_dataset[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data is an image scaled between 0 and 255 and each example is a 2D matrix. We need to scale the data and unwrap each matrix into a 1D vector, where each pixel is a feature for our model. We can do the unwrapping during training, so let's just scale the data for now. \n",
    "\n",
    "Take a look at `transforms` that was imported via `from torchvision import datasets, transforms` above for simple scaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete me\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Build an autoencoder:\n",
    "\n",
    "Our **encoder** will consist of three hidden layers: 128 nodes, 64 nodes, and a final latent space of 3 nodes.\n",
    "Our **decoder** will reverse this process with three layers: 64 nodes, 128 nodes, and 784 (28*28) nodes to recover the dimensions of the original data.\n",
    "\n",
    "We can use a ReLU activation function because our network is fairly deep.\n",
    "\n",
    "\n",
    "![NN](nn.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We follow the same class structure as above, but we need to be able to access the encoder and decoder seperately\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # we need a seperate encoder and decoder object \n",
    "        self.encoder = nn.Sequential(\n",
    "             #complete me\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            # complete me\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # complete me\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Autoencoder\n",
    "\n",
    "Modify the simple taining script below to plot loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = #complete me\n",
    "loss_function = # complete me\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3) # We will use a fancier gradient descent optimizer here\n",
    "\n",
    "for epoch in range(5):  # keep it fast for now\n",
    "    # we are going to train our model in batches because our dataset is large.\n",
    "    for data, _ in train_loader:\n",
    "        data = data.view(data.size(0), -1)  # Flatten\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/5], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(train_loader)\n",
    "    images, _ = next(dataiter)\n",
    "    images_flat = images.view(images.size(0), -1)\n",
    "    reconstructed = model(images_flat)\n",
    "    reconstructed = reconstructed.view(-1, 1, 28, 28)\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(12, 3))\n",
    "    for i in range(8):\n",
    "        axes[0, i].imshow(images[i][0], cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].imshow(reconstructed[i][0], cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "    axes[0, 0].set_ylabel('Original')\n",
    "    axes[1, 0].set_ylabel('Reconstructed')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune your model until you get decent reconstruction, then submit to Gradescope!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python-3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
